import torch
from torch import nn
from torch import optim
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import torch.nn.functional as F

data = pd.read_csv('dataset.csv')
# print(data.info())
# print(data.describe())
# print(data.head())

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = torch.Tensor(X_train)
y_train = torch.Tensor(y_train)
print(X_train.shape)  # torch.Size([200, 1])
print(y_train.shape)  # torch.Size([200])

X_test = torch.Tensor(X_test)

y_test = torch.Tensor(y_test)
print(X_test.shape)  # torch.Size([200])
print(y_test.shape)  # torch.Size([50])

plt.scatter(X_train, y_train)
plt.scatter(X_test, y_test)
# plt.show()

# Build the NN Model
model = nn.Linear(1, 1)
# print(model)
criterion = nn.MSELoss()
# print(model.parameters())
# for name,param in model.named_parameters():
#     print(name,param)
# output for :
# weight Parameter containing:
# tensor([[0.4809]], requires_grad=True)
# bias Parameter containing:
# tensor([-0.1293], requires_grad=True)

optimizer = optim.SGD(model.parameters(), lr=0.01)

# Train Loop
N = 200
for iteration in range(N):
    yp = model(X_train)
    loss = criterion(yp.squeeze(), y_train)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(loss.item())
# Output loss:
# 42.65182113647461
# 40.945098876953125
# 39.30860137939453
# 37.73944854736328
# 36.23486328125
# 34.792179107666016
# 33.40885543823242
# 32.082435607910156
# 30.81058120727539
# 29.591045379638672
# 28.421672821044922
# 27.300395965576172
# 26.22522735595703
# 25.194276809692383
# 24.20572280883789
# 23.2578182220459
# 22.34888458251953
# 21.477319717407227
# 20.641590118408203
# 19.840213775634766
# 19.071779251098633
# 18.33493423461914
# 17.628368377685547
# 16.950841903686523
# 16.301162719726562
# 15.678177833557129
# 15.080793380737305
# 14.507953643798828
# 13.95865249633789
# 13.431915283203125
# 12.9268159866333
# 12.442468643188477
# 11.97801399230957
# 11.532634735107422
# 11.105547904968262
# 10.696002006530762
# 10.303274154663086
# 9.926671028137207
# 9.565530776977539
# 9.219219207763672
# 8.887125015258789
# 8.56866455078125
# 8.263275146484375
# 7.970422267913818
# 7.68958854675293
# 7.420282363891602
# 7.162026882171631
# 6.914369106292725
# 6.676873683929443
# 6.449123382568359
# 6.230717182159424
# 6.02126932144165
# 5.8204169273376465
# 5.627801418304443
# 5.443087100982666
# 5.265949249267578
# 5.096075534820557
# 4.933168888092041
# 4.776942729949951
# 4.627122402191162
# 4.483444690704346
# 4.345657825469971
# 4.213519096374512
# 4.086797714233398
# 3.9652717113494873
# 3.8487257957458496
# 3.736957311630249
# 3.6297690868377686
# 3.5269742012023926
# 3.428391218185425
# 3.333847761154175
# 3.2431774139404297
# 3.1562225818634033
# 3.0728299617767334
# 2.9928536415100098
# 2.9161531925201416
# 2.8425939083099365
# 2.772047996520996
# 2.70439076423645
# 2.6395044326782227
# 2.577274799346924
# 2.5175933837890625
# 2.460355758666992
# 2.405461311340332
# 2.352814197540283
# 2.3023221492767334
# 2.253896713256836
# 2.207453727722168
# 2.1629111766815186
# 2.1201915740966797
# 2.0792202949523926
# 2.0399253368377686
# 2.0022380352020264
# 1.966092824935913
# 1.9314262866973877
# 1.8981777429580688
# 1.8662898540496826
# 1.8357059955596924
# 1.806373119354248
# 1.7782398462295532
# 1.7512565851211548
# 1.7253775596618652
# 1.7005561590194702
# 1.6767494678497314
# 1.6539158821105957
# 1.6320159435272217
# 1.6110115051269531
# 1.5908654928207397
# 1.5715432167053223
# 1.5530105829238892
# 1.5352351665496826
# 1.518186330795288
# 1.5018342733383179
# 1.4861500263214111
# 1.4711064100265503
# 1.4566776752471924
# 1.442838191986084
# 1.4295638799667358
# 1.4168320894241333
# 1.4046204090118408
# 1.3929072618484497
# 1.3816725015640259
# 1.3708964586257935
# 1.3605607748031616
# 1.3506466150283813
# 1.3411375284194946
# 1.3320165872573853
# 1.323267936706543
# 1.3148761987686157
# 1.3068269491195679
# 1.299106478691101
# 1.291700839996338
# 1.2845975160598755
# 1.2777836322784424
# 1.2712483406066895
# 1.264979600906372
# 1.2589666843414307
# 1.2531991004943848
# 1.247666358947754
# 1.2423595190048218
# 1.2372691631317139
# 1.2323863506317139
# 1.2277027368545532
# 1.223210096359253
# 1.2189005613327026
# 1.2147667407989502
# 1.210801601409912
# 1.2069976329803467
# 1.2033491134643555
# 1.1998493671417236
# 1.1964921951293945
# 1.193271517753601
# 1.1901823282241821
# 1.1872191429138184
# 1.1843767166137695
# 1.181649923324585
# 1.1790342330932617
# 1.1765251159667969
# 1.174118161201477
# 1.171809196472168
# 1.1695942878723145
# 1.1674697399139404
# 1.1654316186904907
# 1.163476586341858
# 1.1616010665893555
# 1.159801959991455
# 1.1580760478973389
# 1.1564204692840576
# 1.154832363128662
# 1.1533087491989136
# 1.1518471240997314
# 1.1504451036453247
# 1.1491001844406128
# 1.1478098630905151
# 1.146572232246399
# 1.1453847885131836
# 1.1442457437515259
# 1.1431529521942139
# 1.1421047449111938
# 1.141099214553833
# 1.1401342153549194
# 1.1392087936401367
# 1.138321042060852
# 1.1374691724777222
# 1.1366521120071411
# 1.1358681917190552
# 1.1351161003112793
# 1.134394645690918
# 1.1337025165557861
# 1.1330385208129883
# 1.132401466369629
# 1.131790280342102
# 1.1312041282653809
# 1.1306415796279907
# 1.1301020383834839
# 1.1295843124389648
# 1.1290878057479858
# 1.128611445426941
# 1.1281542778015137
# 1.1277157068252563

# Visualiz
x_visual = torch.linspace(X_train.min(), X_train.max(), 250).unsqueeze(1)
y_visual = model(x_visual)
plt.scatter(X_train, y_train)
plt.plot(x_visual.detach(), y_visual.detach(), 'b')
plt.show()

# Test model
y_prediction = model(X_test)
print(X_test.size())#torch.Size([50, 1])
print(y_prediction.size())#torch.Size([50, 1])


print(F.l1_loss(y_prediction.squeeze(), y_test).item())#0.7454901933670044




